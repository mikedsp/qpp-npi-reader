{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20e1d168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (124.0.6367.91) detected in PATH at C:\\Windows\\chromedriver.exe might not be compatible with the detected chrome version (125.0.6422.76); currently, chromedriver 125.0.6422.78 is recommended for chrome 125.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the NPI text file\n",
      "Number of NPI numbers: 4\n",
      "\n",
      "Beginning HTML scrape for NPI 1306927249 for the year 2022\n",
      "Year:  2022\n",
      "Page has NOT stabilized.\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1306927249', 'Success', 2022, 1, 'MARK GERALD BROOKS', 'MARK G BROOKS M D P A', '10000 W COLONIAL DRIVE SUITE 187, OCOEE, FL 34761-3498', 'MIPS Exempt Individual', 'MIPS Exempt Group', 'None', 'This clinician is not required to report because they are a Qualifying APM Participant (QP)', 'This clinician may voluntarily report as an individual or group, and receive performance feedback.', 'This clinician is eligible to receive a 5% APM Incentive Payment. If they voluntarily report this will not impact their 5% APM Incentive payment.', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '1', 'Check APM Requirements', 'MARK GERALD BROOKS is a participant in 1 APM entity at this practice, and may need to submit data in this system as part of APM specific reporting requirements.', 'Select Physicians Associates, LLC', 'MIPS APM & Advanced APM', 'MEDICARE SHARED SAVINGS PROGRAM ACCOUNTABLE CARE ORGANIZATIONS / MSSP ACO - BASIC LEVEL E', 'This clinician was actively participating in an APM on a snapshot date. Therefore the clinician will be assessed for QP status if in an Advanced APM and is eligible to report via the APP, unless otherwise exempt from MIPS.', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No']\n",
      "Scrape complete for NPI 1306927249 for the year 2022\n",
      "processed_npi:  1\n",
      "remaining_npi:  11\n",
      "Processed: 1/12, Remaining: 11\n",
      "Average processing time per NPI: 5.13 seconds\n",
      "Estimated remaining time: 0.94 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1306927249 for the year 2023\n",
      "Year:  2023\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1306927249', 'Success', 2023, 1, 'MARK GERALD BROOKS', 'MARK G BROOKS M D P A', '10000 W COLONIAL DRIVE SUITE 187, OCOEE, FL 34761-3498', 'MIPS Exempt Individual', 'MIPS Exempt Group', 'None', 'This clinician is not required to report because they are a Qualifying APM Participant (QP).', 'This clinician may voluntarily report traditional MIPS.', \"2023: This clinician is eligible to receive a 3.5% APM Incentive Payment. Voluntary reporting won't impact their 3.5% incentive payment.2024: The clinician will receive a Physician Fee Schedule update on services furnished in the payment year based on the 0.75 percent qualifying APM conversion factor. Voluntary reporting won't affect this.\", 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '1', 'Check APM Requirements', 'MARK GERALD BROOKS is a participant in 1 APM entity at this practice, and may need to submit data in this system as part of APM specific reporting requirements.', 'Select Physicians Associates, LLC', 'MIPS APM & Advanced APM', 'MEDICARE SHARED SAVINGS PROGRAM / MSSP ACO - ENHANCED', 'This clinician was actively participating in the APM on a snapshot date. Therefore the clinician will be assessed for QP if in an Advanced APM and/or is eligible to report via the APP.', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No']\n",
      "Scrape complete for NPI 1306927249 for the year 2023\n",
      "processed_npi:  2\n",
      "remaining_npi:  10\n",
      "Processed: 2/12, Remaining: 10\n",
      "Average processing time per NPI: 3.55 seconds\n",
      "Estimated remaining time: 0.59 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1306927249 for the year 2024\n",
      "Year:  2024\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1306927249', 'Success', 2024, 1, 'MARK GERALD BROOKS', 'MARK G BROOKS M D P A', '10000 W COLONIAL DRIVE SUITE 187, OCOEE, FL 34761-3498', 'MIPS Eligible Individual', 'MIPS Eligible Group', 'None', \"This clinician is required to report because they're a MIPS eligible clinician type, enrolled in Medicare before the performance year, and exceed the individual low-volume threshold.\", 'This clinician is a MIPS APM participant, and can report the APM Performance Pathway (APP), traditional MIPS or a MIPS Value Pathway (MVP), participating as an individual, as part of a group, or part of an APM Entity.If reporting an MVP, this clinician may also participate as part of a subgroup.Advance registration required to report an MVP.', 'This clinician will receive the MIPS payment adjustment associated with the highest final score available to them at this practice - from individual, group, APM Entity or subgroup participation.', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', 'Yes', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '1', 'Check APM Requirements', 'MARK GERALD BROOKS is a participant in 1 APM entity at this practice, and may need to submit data in this system as part of APM specific reporting requirements.', 'Select Physicians Associates, LLC', 'MIPS APM & Advanced APM', 'MEDICARE SHARED SAVINGS PROGRAM / MSSP ACO - ENHANCED', 'This clinician was actively participating in the APM on a snapshot date. Therefore the clinician will be assessed for QP if in an Advanced APM and/or is eligible to report via the APP.', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No']\n",
      "Scrape complete for NPI 1306927249 for the year 2024\n",
      "processed_npi:  3\n",
      "remaining_npi:  9\n",
      "Processed: 3/12, Remaining: 9\n",
      "Average processing time per NPI: 3.17 seconds\n",
      "Estimated remaining time: 0.48 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1891745576 for the year 2022\n",
      "Year:  2022\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Specific error message found: The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1891745576', 'The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.', 2022]\n",
      "Scrape complete for NPI 1891745576 for the year 2022\n",
      "processed_npi:  4\n",
      "remaining_npi:  8\n",
      "Processed: 4/12, Remaining: 8\n",
      "Average processing time per NPI: 2.84 seconds\n",
      "Estimated remaining time: 0.38 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1891745576 for the year 2023\n",
      "Year:  2023\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Specific error message found: The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1891745576', 'The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.', 2023]\n",
      "Scrape complete for NPI 1891745576 for the year 2023\n",
      "processed_npi:  5\n",
      "remaining_npi:  7\n",
      "Processed: 5/12, Remaining: 7\n",
      "Average processing time per NPI: 2.79 seconds\n",
      "Estimated remaining time: 0.33 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1891745576 for the year 2024\n",
      "Year:  2024\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Specific error message found: The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1891745576', 'The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.', 2024]\n",
      "Scrape complete for NPI 1891745576 for the year 2024\n",
      "processed_npi:  6\n",
      "remaining_npi:  6\n",
      "Processed: 6/12, Remaining: 6\n",
      "Average processing time per NPI: 2.61 seconds\n",
      "Estimated remaining time: 0.26 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1720084114 for the year 2022\n",
      "Year:  2022\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 2\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1720084114', 'Success', 2022, 2, 'MICHAEL L MAWBY', 'INFUSION ASSOCIATES PLLC', '44720 HAYES RD STE 200, CLINTON TOWNSHIP, MI 48038-1091', 'MIPS Eligible Individual', 'MIPS Eligible Group', 'None', 'This clinician is required to report because they are a MIPS eligible clinician type, have been enrolled in Medicare for greater than a year, and exceed the individual low-volume threshold.', 'This clinician can report as part of a group, or as an individual, or both ways. Learn more about this choice.', 'If the practice reports as a group, this clinician will receive a payment adjustment based on the group score. If they report as an individual, they will receive a payment adjustment based on their individual score. If they report in both ways, the clinician will receive a payment adjustment based on the higher of the two scores.', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '0', 'Not Required to Report for any APMs', '', '', '', '', '', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No']\n",
      "Scrape complete for NPI 1720084114 for the year 2022\n",
      "processed_npi:  7\n",
      "remaining_npi:  5\n",
      "Processed: 7/12, Remaining: 5\n",
      "Average processing time per NPI: 2.64 seconds\n",
      "Estimated remaining time: 0.22 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1720084114 for the year 2023\n",
      "Year:  2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Specific error message found: 2023 is not a supported performance year\n",
      "filename:  errors\\1720084114.html\n",
      "HTML content has been written to errors\\1720084114.html for further inspection.\n",
      "\n",
      "\n",
      "Error message found!\n",
      "Attempt 1: Unsupported performance year found. Retrying...\n",
      "Beginning HTML scrape for NPI 1720084114 for the year 2023\n",
      "Year:  2023\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1720084114', 'Success', 2023, 1, 'MICHAEL L MAWBY', 'INFUSION ASSOCIATES PLLC', '44720 HAYES RD STE 200, CLINTON TOWNSHIP, MI 48038-1091', 'MIPS Eligible Individual', 'MIPS Eligible Group', 'None', \"This clinician is required to report because they're a MIPS eligible clinician type, enrolled in Medicare before the performance year, and exceed the individual low-volume threshold.\", 'This clinician is a MIPS APM participant, and can report the APM Performance Pathway (APP), traditional MIPS or a MIPS Value Pathway (MVP), participating as an individual, as part of a group, or part of an APM Entity.If reporting an MVP, this clinician may also participate as part of a subgroup.Advance registration required to report an MVP.', 'This clinician will receive the MIPS payment adjustment associated with the highest final score available to them at this practice - from individual, group, APM Entity or subgroup participation.', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '1', 'Check APM Requirements', 'MICHAEL L MAWBY is a participant in 1 APM entity at this practice, and may need to submit data in this system as part of APM specific reporting requirements.', 'Renovis Health LLC', 'MIPS APM & Advanced APM', 'ACO REACH MODEL / ACO REACH - GLOBAL', 'This clinician is a preferred provider and must report traditional MIPS or an MVP, unless otherwise exempt. This clinician is not eligible to report via the APP and will not be included in QP determinations.', 'No', 'No', 'No', 'No', 'No', 'Extreme and uncontrollable circumstances', 'No', 'No', 'No', 'No']\n",
      "Scrape complete for NPI 1720084114 for the year 2023\n",
      "processed_npi:  8\n",
      "remaining_npi:  4\n",
      "Processed: 8/12, Remaining: 4\n",
      "Average processing time per NPI: 2.57 seconds\n",
      "Estimated remaining time: 0.17 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1720084114 for the year 2024\n",
      "Year:  2024\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1720084114', 'Success', 2024, 1, 'MICHAEL L MAWBY', 'INFUSION ASSOCIATES PLLC', '44720 HAYES RD STE 200, CLINTON TOWNSHIP, MI 48038-1091', 'MIPS Eligible Individual', 'MIPS Eligible Group', 'None', \"This clinician is required to report because they're a MIPS eligible clinician type, enrolled in Medicare before the performance year, and exceed the individual low-volume threshold.\", 'This clinician can report traditional MIPS or a MIPS Value Pathway (MVP), participating as an individual or as part of a group.If reporting an MVP, this clinician may also participate as part of a subgroup.Advance registration required to report an MVP.', 'This clinician will receive the MIPS payment adjustment associated with the highest final score available to them at this practice - from individual, group or subgroup participation.', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', 'Yes', 'Yes', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '0', 'Not Required to Report for any APMs', '', '', '', '', '', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
      "Scrape complete for NPI 1720084114 for the year 2024\n",
      "processed_npi:  9\n",
      "remaining_npi:  3\n",
      "Processed: 9/12, Remaining: 3\n",
      "Average processing time per NPI: 2.55 seconds\n",
      "Estimated remaining time: 0.13 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1922736404 for the year 2022\n",
      "Year:  2022\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1922736404', 'Success', 2022, 1, 'BENTLEY K HILL', 'EYE PARTNERS PC', '121 SAINT JOSEPH AVE, BREWTON, AL 36426-2021', 'MIPS Exempt Individual', 'MIPS Exempt Group', 'None', 'This clinician is not required to report because they haven’t been enrolled in Medicare for greater than a year.', 'This clinician may voluntarily report as an individual or part of a group.', 'If this clinician voluntarily reports, they will not receive a payment adjustment.', 'No', 'Does not exceed 200', 'Does not exceed $90,000', 'Does not exceed 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '0', 'Not Required to Report for any APMs', '', '', '', '', '', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']\n",
      "Scrape complete for NPI 1922736404 for the year 2022\n",
      "processed_npi:  10\n",
      "remaining_npi:  2\n",
      "Processed: 10/12, Remaining: 2\n",
      "Average processing time per NPI: 2.52 seconds\n",
      "Estimated remaining time: 0.08 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1922736404 for the year 2023\n",
      "Year:  2023\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1922736404', 'Success', 2023, 1, 'BENTLEY K HILL', 'EYE PARTNERS PC', '121 SAINT JOSEPH AVE, BREWTON, AL 36426-2021', 'MIPS Exempt Individual', 'MIPS Eligible Group', 'None', 'This clinician is not required to report because they do not exceed the individual low-volume threshold. If the practice reports as a group, however, their data will be included.', 'This clinician can voluntarily report traditional MIPS as an individual.The practice can report traditional MIPS or a MIPS Value Pathway (MVP) as a group.This clinician can report an MVP as part of a subgroup.Advance registration required to report an MVP.', \"This clinician will receive a MIPS payment adjustment if reporting as part of a group or subgroup.This clinician won't receive a MIPS payment adjustment from voluntary reporting as an individual.\", 'No', 'Does not exceed 200', 'Does not exceed $90,000', 'Does not exceed 200', 'Yes', '', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '0', 'Not Required to Report for any APMs', '', '', '', '', '', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes']\n",
      "Scrape complete for NPI 1922736404 for the year 2023\n",
      "processed_npi:  11\n",
      "remaining_npi:  1\n",
      "Processed: 11/12, Remaining: 1\n",
      "Average processing time per NPI: 2.47 seconds\n",
      "Estimated remaining time: 0.04 minutes\n",
      "\n",
      "Beginning HTML scrape for NPI 1922736404 for the year 2024\n",
      "Year:  2024\n",
      "Page has NOT stabilized.\n",
      "Page has stabilized.\n",
      "Number of Associated Practices: 1\n",
      "\n",
      "\n",
      "Did not detect Not a Supported Performance Year\n",
      "\n",
      "Data to write to csv = ['1922736404', 'Success', 2024, 1, 'BENTLEY K HILL', 'EYE PARTNERS PC', '121 SAINT JOSEPH AVE, BREWTON, AL 36426-2021', 'MIPS Exempt Individual', 'MIPS Eligible Group', 'Opt-in eligibleas individual', \"This clinician is not required to report because they do not exceed the individual low-volume threshold. Because they exceed one or two elements of the low-volume threshold, however, they can opt-in to report as an individual. In either case, because the practice exceeds the low-volume threshold, if the practice reports as a group, the clinician's data will be included.\", 'This clinician can voluntarily report, or opt-in to report, traditional MIPS as an individual.The practice can report traditional MIPS or a MIPS Value Pathway (MVP) as a group.This clinician can report an MVP as part of a subgroup.Advance registration required to report an MVP.', \"This clinician will receive a MIPS payment adjustment if reporting as part of a group or subgroup, or if they opt in and report as an individual.This clinician won't receive a MIPS payment adjustment from voluntary reporting as an individual.\", 'No', 'Does not exceed 200', 'Does not exceed $90,000', 'Exceeds 200', 'Yes', 'Yes', 'Yes', 'Exceeds 200', 'Exceeds $90,000', 'Exceeds 200', '0', 'Not Required to Report for any APMs', '', '', '', '', '', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes']\n",
      "Scrape complete for NPI 1922736404 for the year 2024\n",
      "processed_npi:  12\n",
      "remaining_npi:  0\n",
      "Processed: 12/12, Remaining: 0\n",
      "Average processing time per NPI: 2.42 seconds\n",
      "Estimated remaining time: 0.00 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Complete\n"
     ]
    }
   ],
   "source": [
    "## NPI Reader - 10, May 24, 2024\n",
    "## To specify the input file and which years to extract, set the variables defined near line 900\n",
    "\n",
    "## Kicks out on any of 3 errors found: pecos, no claims data, unsupported year\n",
    "#\n",
    "# In the function that calls fetch_info, have it catch if the unsupported year occurred and if it did, have it\n",
    "# try calling the function 5 more times\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def fetch_info(npi, year):\n",
    "    #print(\"Input npi to fetch_info() = \", npi)\n",
    "    #url = f\"https://qpp.cms.gov/participation-lookup?npi={npi}&py=2024\"\n",
    "    #url = f\"https://qpp.cms.gov/participation-lookup?npi={npi}&py=2023\"\n",
    "    print(\"Year: \", year)\n",
    "    url = f\"https://qpp.cms.gov/participation-lookup?npi={npi}&py={year}\"\n",
    "   \n",
    "    # Initialize all the variable fields that we'll be attempting to parse. If the field can't be parsed, then \"temp\" will be returned and the function won't error out\n",
    "    npi_value = \"temp\"\n",
    "    associated_practices = \"temp\"\n",
    "    physician_name = \"temp\"\n",
    "    practice_name = \"temp\"\n",
    "    practice_address = \"temp\"\n",
    "    mips_info = \"temp\"\n",
    "    individual_status = \"temp\"\n",
    "    group_status = \"temp\"\n",
    "    optin_text = \"temp\"\n",
    "    mips_reporting_requirements = \"temp\"\n",
    "    mips_reporting_options = \"temp\"\n",
    "    payment_information = \"temp\"\n",
    "    cl_exceeds_low_volume = \"temp\" \n",
    "    cl_med_patients = \"temp\"\n",
    "    cl_allowed_charges = \"temp\"\n",
    "    cl_covered_services = \"temp\"\n",
    "    cl_mips_ec_type = \"temp\"\n",
    "    cl_enroll_b4_2024 = \"\"\n",
    "    pl_exceed_lv = \"temp\"\n",
    "    pl_medicare_pts = \"temp\"\n",
    "    pl_allowed_charges = \"temp\"\n",
    "    apm_participation = \"temp\" \n",
    "    apm_participation_num = \"temp\"\n",
    "    apm_participation_text = \"temp\"\n",
    "    apm_participation_comment = \"temp\"\n",
    "    apm_name = \"temp\"\n",
    "    apm_details_classification = \"\"   # if APM Details (Classification) doesn't exist, put an empty string in the csv file\n",
    "    apm_details_model = \"\"   # if APM Details (Model) doesn't exist, put an empty string in the csv file\n",
    "    apm_details_participation_details = \"\"   # if APM Details (Participation Details) doesn't exist, put an empty string in the csv file\n",
    "    data_fetch_status = \"Success\"\n",
    "    cl_hospital_based_status = \"No\" \n",
    "    cl_npfacing_status = \"No\" \n",
    "    cl_rural_status = \"No\" \n",
    "    cl_small_practice_status = \"No\" \n",
    "    pl_hospital_based_status = \"No\" \n",
    "    pl_rural_status = \"No\" \n",
    "    pl_small_practice_status = \"No\" \n",
    "    cl_hspa_status = \"No\"\n",
    "    pl_hpsa_status = \"No\"\n",
    "    cl_hardship = \"No\"  \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        \n",
    "\n",
    "        # Function to get the hash of the page source\n",
    "        def get_page_hash():\n",
    "            return hashlib.md5(driver.page_source.encode('utf-8')).hexdigest()   \n",
    "        # Initialize old and new hashes\n",
    "        old_page_hash = None\n",
    "        new_page_hash = get_page_hash()   \n",
    "        # Initialize start time for the loop\n",
    "        start_time = time.time()\n",
    "        timeout = 30  # seconds    \n",
    "        # Loop until the page source is the same between checks or timeout occurs\n",
    "        while time.time() - start_time < timeout:\n",
    "            if old_page_hash == new_page_hash:\n",
    "                print(\"Page has stabilized.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Page has NOT stabilized.\")\n",
    "                time.sleep(1)  # Wait for 2 seconds before rechecking\n",
    "                old_page_hash = new_page_hash\n",
    "                new_page_hash = get_page_hash()\n",
    "    \n",
    "        # Check if the loop exited due to a timeout\n",
    "        if old_page_hash != new_page_hash:\n",
    "            print(\"Page did not stabilize within the timeout period.\")\n",
    "            raise TimeoutException(\"Page did not stabilize within the timeout period.\")\n",
    "\n",
    "        \n",
    "        # Use this section to output the complete generated html file\n",
    "        #print(\"############### START OF HTMLDUMP ##############################\")\n",
    "        #print()\n",
    "        html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "        #print(html) # the preceding line captured the entire dynamically generated html\n",
    "        #print()\n",
    "        #print(\"############### END OF HTMLDUMP ##############################\")\n",
    "        #print()\n",
    "\n",
    "    \n",
    "        # mike - let's see if we get to here!\n",
    "        #print(driver)\n",
    "       \n",
    "       # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(html, 'html.parser')     \n",
    "        \n",
    "        # Look for all instances of <div class=\"qpp-c-alert__body\">\n",
    "        error_divs = soup.find_all('div', class_=\"qpp-c-alert__body\")\n",
    "        expected_error_message = \"The National Provider Identifier (NPI) does not have claims data for this given period.\"\n",
    "        expected_error_message_pecos = \"The National Provider Identifier (NPI) does not exist in the PECOS import yet. Please try again later.\"\n",
    "        expected_error_message_not_supported = \"is not a supported performance year\"\n",
    "\n",
    "        \n",
    "        # Iterate over each found error div and check if it contains the expected text\n",
    "        for error_div in error_divs:\n",
    "            if expected_error_message in error_div.get_text(strip=True):\n",
    "                print(\"Specific error message found:\", error_div.get_text(strip=True))\n",
    "                return [npi, error_div.get_text(strip=True), year]\n",
    "            if expected_error_message_pecos in error_div.get_text(strip=True):\n",
    "                print(\"Specific error message found:\", error_div.get_text(strip=True))\n",
    "                return [npi, error_div.get_text(strip=True), year]\n",
    "            if expected_error_message_not_supported in error_div.get_text(strip=True):\n",
    "                print(\"Specific error message found:\", error_div.get_text(strip=True))\n",
    "                data_fetch_status = \"NPI section not found.\"\n",
    "                #print(html)\n",
    "                # Check if the errors folder exists, create it if not\n",
    "                error_dir = 'errors'\n",
    "                if not os.path.exists(error_dir):\n",
    "                    os.makedirs(error_dir)\n",
    "\n",
    "                ## When npi_div is not found, save the HTML to a file within the errors subfolder\n",
    "                filename = os.path.join(error_dir, f\"{npi}.html\")\n",
    "                print(\"filename: \", filename)\n",
    "                with open(filename, 'w', encoding='utf-8') as file:\n",
    "                    file.write(html)  # Write the HTML content to the file\n",
    "                print(f\"HTML content has been written to {filename} for further inspection.\")                \n",
    "                return [npi, error_div.get_text(strip=True), year]\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        # Locate the <div> with class 'npi'\n",
    "        npi_div = soup.find('div', class_='npi')\n",
    "        #print(\"npi_div =\", npi_div)\n",
    "        if npi_div:\n",
    "            npi_value = npi_div.get_text(strip=True)  # Extract the text content, stripping whitespace\n",
    "            #print(\"NPI: \", npi_value)\n",
    "        else:\n",
    "            print(\"NPI section not found.\")\n",
    "            data_fetch_status = \"NPI section not found.\"\n",
    "            #print(html)\n",
    "            # Check if the errors folder exists, create it if not\n",
    "            error_dir = 'errors'\n",
    "            if not os.path.exists(error_dir):\n",
    "                os.makedirs(error_dir)\n",
    "\n",
    "            ## When npi_div is not found, save the HTML to a file within the errors subfolder\n",
    "            filename = os.path.join(error_dir, f\"{npi}.html\")\n",
    "            print(\"filename: \", filename)\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(html)  # Write the HTML content to the file\n",
    "            print(f\"HTML content has been written to {filename} for further inspection.\")\n",
    "            return [npi, data_fetch_status, year]  \n",
    "            \n",
    "       ## extract Associated Practices\n",
    "       # Find the section with class 'practice-details'\n",
    "        section = soup.find('section', class_='practice-details')\n",
    "\n",
    "        # Initialize a variable to store the extracted number\n",
    "        number_of_practices = None\n",
    "\n",
    "        # Find the h4 tag and extract the number\n",
    "        if section:\n",
    "            h4_text = section.find('h4').get_text()  # Get the text of the h4 tag\n",
    "            # Extract the number from the string\n",
    "            start = h4_text.find('(') + 1\n",
    "            end = h4_text.find(')')\n",
    "            if start > 0 and end > 0:\n",
    "                associated_practices = int(h4_text[start:end])  # Convert the substring to an integer\n",
    "        print(\"Number of Associated Practices:\", associated_practices)\n",
    "            \n",
    "            \n",
    "            \n",
    "        ## Extract physician name, practice name, and practice address    \n",
    "        practice_header = soup.find('div', class_='practice-header')\n",
    "        if practice_header:\n",
    "            # Extracting the physician's name, practice name, and practice address\n",
    "            physician_name_untruncated = practice_header.find('h5').get_text(strip=True)\n",
    "            physician_name = physician_name_untruncated.split(\" at\")[0]   #The html is of the form \"Phys name at Practice Name\"\n",
    "            practice_name = practice_header.find('span').next_sibling.strip()\n",
    "            practice_address = practice_header.find('div', class_='address').get_text(strip=True)\n",
    "        \n",
    "            #print(\"MIPS Individual: \", eligibility_info )\n",
    " \n",
    "            #print(\"Physician Name: \", physician_name)\n",
    "            #print(\"Practice  Name: \",practice_name)\n",
    "            #print(\"Practice Address: \",practice_address)        \n",
    "        else:\n",
    "            data_fetch_status = \"Practice header section not found\"\n",
    "            return [npi, data_fetch_status]  \n",
    "        \n",
    "        \n",
    "        ## Extract whether Whether a physician is classified as “MIPS Eligible Individual” or “MIPS Exempt Individual” and whether a physician is classified as “MIPS Eligible Group” or “MIPS Exempt Group\"\n",
    "        reporting_requirements_div = soup.find(\"div\", class_=\"reporting-requirements\")\n",
    "        # If the specific div is not found, return an error or unknown status\n",
    "        if not reporting_requirements_div:\n",
    "            return {\"Individual Status\": \"Unknown\", \"Group Status\": \"Unknown\"}\n",
    "    \n",
    "        individual_span = reporting_requirements_div.find(\"span\", {\"aria-label\": lambda value: value and \"Individual\" in value})\n",
    "        group_span = reporting_requirements_div.find(\"span\", {\"aria-label\": lambda value: value and \"Group\" in value})\n",
    "    \n",
    "        individual_status = individual_span['aria-label'] if individual_span else \"Unknown\"\n",
    "        group_status = group_span['aria-label'] if group_span else \"Unknown\"\n",
    "        #print(\"individual_status: \", individual_status)  \n",
    "        #print(\"group_status: \", group_status)       \n",
    "        \n",
    "        \n",
    "        ## Extract the Opt In Option\n",
    "        opt_in_div = soup.find(\"div\", class_=\"opt-in-flag\") \n",
    "        if opt_in_div:\n",
    "            optin_text = opt_in_div.get_text(strip=True)\n",
    "            prefix = \"Opt-in Option:\"\n",
    "            if optin_text.startswith(prefix):\n",
    "                optin_text = optin_text[len(prefix):]  # Remove the prefix from the start\n",
    "        else:\n",
    "            optin_text = \"None\"\n",
    "        #print(\"optin_text:\", optin_text)\n",
    "        \n",
    "        ## Scrape contents of 'MIPS Reporting Requirements'\n",
    "        # Locate all 'div' elements with class 'reporting-table-label' and check their text\n",
    "        table_labels = soup.find_all(\"div\", class_=\"reporting-table-label\")\n",
    "        for label in table_labels:\n",
    "            if label.get_text(strip=True) == \"MIPS Reporting Requirements\":\n",
    "                # Get the next sibling 'div' element which is 'reporting-table-field'\n",
    "                content_div = label.find_next_sibling(\"div\", class_=\"reporting-table-field\")\n",
    "                if content_div:\n",
    "                    inner_content = content_div.find(\"div\", class_=\"reporting-inner-content\")\n",
    "                    mips_reporting_requirements = inner_content.get_text(strip=True) if inner_content else \"Content not found\"\n",
    "                    break  # Stop searching once the correct section is found\n",
    "        #print(\"mips_reporting_requirements:\", mips_reporting_requirements)\n",
    "\n",
    "        ## Scrape contents of 'MIPS REPORTING & PARTICIPATION OPTIONS'\n",
    "        # Locate all 'div' elements with class 'reporting-table-label' and check their text\n",
    "        table_labels = soup.find_all(\"div\", class_=\"reporting-table-label\")\n",
    "        for label in table_labels:\n",
    "            if label.get_text(strip=True) == \"MIPS Reporting & Participation Options\":\n",
    "                # Get the next sibling 'div' element which is 'reporting-table-field'\n",
    "                content_div = label.find_next_sibling(\"div\", class_=\"reporting-table-field\")\n",
    "                if content_div:\n",
    "                    inner_content = content_div.find(\"div\", class_=\"reporting-inner-content\")\n",
    "                    mips_reporting_options = inner_content.get_text(strip=True) if inner_content else \"Content not found\"\n",
    "                    break  # Stop searching once the correct section is found\n",
    "        #print(\"mips_reporting_options:\", mips_reporting_options)\n",
    "       \n",
    "        ## Scrape contents of 'Payment Information'\n",
    "        # Locate all 'div' elements with class 'reporting-table-label' and check their text\n",
    "        table_labels = soup.find_all(\"div\", class_=\"reporting-table-label\")\n",
    "        for label in table_labels:\n",
    "            if label.get_text(strip=True) == \"Payment Information\":\n",
    "                # Get the next sibling 'div' element which is 'reporting-table-field'\n",
    "                content_div = label.find_next_sibling(\"div\", class_=\"reporting-table-field\")\n",
    "                if content_div:\n",
    "                    inner_content = content_div.find(\"div\", class_=\"reporting-inner-content\")\n",
    "                    payment_information = inner_content.get_text(strip=True) if inner_content else \"Content not found\"\n",
    "                    break  # Stop searching once the correct section is found\n",
    "        #print(\"payment_information:\", payment_information)\n",
    "\n",
    "        \n",
    "        ## Scrape contents for (Clinician Level) Exceeds low volume threshold\n",
    "        #cl_exceeds_low_volume = \"Information not found\"  # Initialize the variable to hold extracted information\n",
    "        # First locate the 'div' with class 'scenario'\n",
    "        scenario_div = soup.find(\"div\", class_=\"scenario\")\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Exceeds low volume threshold\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_exceeds_low_volume = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_exceeds_low_volume = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_exceeds_low_volume = \"Scenario section not found\"\n",
    "        #print(\"cl_exceeds_low_volume: \", cl_exceeds_low_volume)\n",
    "\n",
    "        \n",
    "        ## Scrape contents for (Clinician Level) Medicare patients for this clinician\n",
    "        # First locate the 'div' with class 'scenario'\n",
    "        scenario_div = soup.find(\"div\", class_=\"scenario\")\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Medicare patients for this clinician\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_med_patients = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_med_patients = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_med_patients = \"Scenario section not found\"\n",
    "        #print(\"cl_med_patients: \", cl_med_patients)\n",
    "\n",
    "        \n",
    "        ## Scrape contents for (Clinician Level) Allowed charges for this clinician\n",
    "        scenario_div = soup.find(\"div\", class_=\"scenario\")\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Allowed charges for this clinician\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_allowed_charges = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_allowed_charges = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_allowed_charges = \"Scenario section not found\"\n",
    "        #print(\"cl_allowed_charges: \", cl_allowed_charges)\n",
    "        \n",
    "        \n",
    "        ## Scrape contents for (Clinician Level) Covered services for this clinician\n",
    "        #scenario_div = soup.find(\"div\", class_=\"scenario\")  # WE GOT THIS IN THE PRECEDING SECTION, NO NEED TO RE_FETCH\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Covered services for this clinician\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_covered_services = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_covered_services = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_covered_services = \"Scenario section not found\"\n",
    "        #print(\"cl_covered_services: \", cl_covered_services)\n",
    "\n",
    "        \n",
    "        ##Scrape contents for (Clinician Level) MIPS eligible clinician type\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"MIPS eligible clinician type\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_mips_ec_type = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_mips_ec_type = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_mips_ec_type = \"Scenario section not found\"\n",
    "        #print(\"cl_mips_ec_type: \", cl_mips_ec_type)\n",
    " \n",
    "\n",
    "        ##Scrape contents for (Clinician Level) Enrolled in Medicare before January 1, 2024\n",
    "        if scenario_div:\n",
    "            # Within 'scenario', locate the specific 'div' for Clinician Level Information\n",
    "            clinician_level_info = scenario_div.find(\"div\", class_=\"h8\", text=\"Clinician Level Information\")\n",
    "            if clinician_level_info:\n",
    "                # Locate all 'div' elements with class 'detail' following the specific 'div'\n",
    "                detail_blocks = clinician_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\").find_all(\"div\", class_=\"detail\")\n",
    "                for block in detail_blocks:\n",
    "                    # Check if the label matches \"Exceeds low volume threshold\"\n",
    "                    label = block.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Enrolled in Medicare before January 1, 2024\" in label.get_text(strip=True):\n",
    "                        field = block.find(\"div\", class_=\"detail-field\")\n",
    "                        cl_enroll_b4_2024 = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Stop searching once the correct section is found\n",
    "            else:\n",
    "                cl_enroll_b4_2024 = \"Clinician level information section not found\"\n",
    "        else:\n",
    "            cl_enroll_b4_2024 = \"Scenario section not found\"\n",
    "        #print(\"cl_enroll_b4_2024: \", cl_enroll_b4_2024)\n",
    "\n",
    "        ## Scrape contents for (Practice Level) Exceeds low volume threshold\n",
    "        # Directly locate the 'h8' div with the specified text, and then navigate to sibling details\n",
    "        practice_level_info = soup.find(\"div\", class_=\"h8\", text=\"Practice Level Information\")\n",
    "        if practice_level_info:\n",
    "            detail_boxed = practice_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\")\n",
    "            if detail_boxed:\n",
    "                details = detail_boxed.find_all(\"div\", class_=\"detail\")\n",
    "                for detail in details:\n",
    "                    label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Exceeds low volume threshold\" in label.get_text(strip=True):\n",
    "                        field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                        pl_exceed_lv = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Found the correct label and extracted the field text\n",
    "            else:\n",
    "                pl_exceed_lv = \"Detail boxed section not found\"\n",
    "        else:\n",
    "            pl_exceed_lv = \"Practice level information section not found\"\n",
    "        #print(\"pl_exceed_lv: \", pl_exceed_lv)\n",
    "\n",
    "        ## Scrape contents for (Practice Level) Medicare patients at this practice\n",
    "        # Directly locate the 'h8' div with the specified text, and then navigate to sibling details\n",
    "        #practice_level_info = soup.find(\"div\", class_=\"h8\", text=\"Practice Level Information\")   #don't need to find this again since it was found in the previous block\n",
    "        if practice_level_info:\n",
    "            detail_boxed = practice_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\")\n",
    "            if detail_boxed:\n",
    "                details = detail_boxed.find_all(\"div\", class_=\"detail\")\n",
    "                for detail in details:\n",
    "                    label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Medicare patients at this practice\" in label.get_text(strip=True):\n",
    "                        field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                        pl_medicare_pts = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Found the correct label and extracted the field text\n",
    "            else:\n",
    "                pl_medicare_pts = \"Detail boxed section not found\"\n",
    "        else:\n",
    "            pl_medicare_pts = \"Practice level information section not found\"\n",
    "        #print(\"pl_medicare_pts: \", pl_medicare_pts)\n",
    "        \n",
    "\n",
    "        ## Scrape contents for (Practice Level) Allowed charges at this practice\n",
    "        # Directly locate the 'h8' div with the specified text, and then navigate to sibling details\n",
    "        #practice_level_info = soup.find(\"div\", class_=\"h8\", text=\"Practice Level Information\")   #don't need to find this again since it was found in the previous block\n",
    "        if practice_level_info:\n",
    "            detail_boxed = practice_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\")\n",
    "            if detail_boxed:\n",
    "                details = detail_boxed.find_all(\"div\", class_=\"detail\")\n",
    "                for detail in details:\n",
    "                    label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Allowed charges at this practice\" in label.get_text(strip=True):\n",
    "                        field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                        pl_allowed_charges = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Found the correct label and extracted the field text\n",
    "            else:\n",
    "                pl_allowed_charges = \"Detail boxed section not found\"\n",
    "        else:\n",
    "            pl_allowed_charges = \"Practice level information section not found\"\n",
    "        #print(\"pl_allowed_charges: \", pl_allowed_charges)\n",
    "\n",
    "        \n",
    "        ## Scrape contents for (Practice Level) Covered services at this practice\n",
    "        # Directly locate the 'h8' div with the specified text, and then navigate to sibling details\n",
    "        #practice_level_info = soup.find(\"div\", class_=\"h8\", text=\"Practice Level Information\")   #don't need to find this again since it was found in the previous block\n",
    "        if practice_level_info:\n",
    "            detail_boxed = practice_level_info.find_next_sibling(\"div\", class_=\"detail-boxed\")\n",
    "            if detail_boxed:\n",
    "                details = detail_boxed.find_all(\"div\", class_=\"detail\")\n",
    "                for detail in details:\n",
    "                    label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                    if label and \"Covered services at this practice\" in label.get_text(strip=True):\n",
    "                        field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                        pl_covered_services = field.get_text(strip=True) if field else \"Field not found\"\n",
    "                        break  # Found the correct label and extracted the field text\n",
    "            else:\n",
    "                pl_covered_services = \"Detail boxed section not found\"\n",
    "        else:\n",
    "            pl_covered_services = \"Practice level information section not found\"\n",
    "        #print(\"pl_covered_services: \", pl_covered_services)\n",
    "        \n",
    "        ## Extract APM Participation (x) info\n",
    "        # Locate the <h6> tag containing the string \"APM Participation\"\n",
    "        h6_tag = soup.find(\"h6\", text=lambda text: text and \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            apm_participation = h6_tag.get_text(strip=True)\n",
    "        else:\n",
    "            apm_participation = \"No <h6> tag with 'APM Participation' found\"\n",
    "        #print(\"apm_participation: \", apm_participation)\n",
    " \n",
    "        ## Extract APM Participation (x) info\n",
    "        # Locate the <h6> tag containing the string \"APM Participation\"\n",
    "        #h6_tag = soup.find(\"h6\", text=lambda text: text and \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            # Extract only the number inside the parentheses using regex\n",
    "            match = re.search(r'\\((\\d+)\\)', h6_tag.get_text())\n",
    "            if match:\n",
    "                apm_participation_num = match.group(1)  # Group 1 is the first captured group, the number inside the parentheses\n",
    "            else:\n",
    "                apm_participation_num = \"No number found in <h6> tag\"\n",
    "        else:\n",
    "            apm_participation_num = \"No <h6> tag with 'APM Participation' found\"   \n",
    "        #print(\"apm_participation_num: \", apm_participation_num)\n",
    "   \n",
    " \n",
    "        ## Extract \"Check APM Requirements\" or \"NOT REQUIRED TO REPORT FOR ANY APMS\"\n",
    "        # we're using the same h6_tag from the previous block, so no need to refind it\n",
    "        #h6_tag = soup.find(\"h6\", text=lambda text: text and \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            # Find the next sibling span with the specific class\n",
    "            span_tag = h6_tag.find_next_sibling(\"span\", class_=\"reporting-requirements no-label\")\n",
    "            if span_tag:\n",
    "                apm_participation_text = span_tag.get_text(strip=True)\n",
    "            else:\n",
    "                apm_participation_text = \"No following span with class 'reporting-requirements no-label' found\"\n",
    "        else:\n",
    "            apm_participation_text = \"No <h6> tag with 'APM Participation' found\"\n",
    "        #print(\"apm_participation_text: \", apm_participation_text)\n",
    " \n",
    "    \n",
    "       ## Extract APM Participation Requirements Comment\n",
    "       # Find the <h6> with 'APM Participation' and limit searches to its parent <section>\n",
    "        h6_tag = soup.find(\"h6\", text=lambda text: \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            section_tag = h6_tag.parent  # Assuming the <h6> is directly under the <section>\n",
    "            span_tag = section_tag.find(\"span\", class_=\"reporting-requirements no-label\")\n",
    "            if span_tag:\n",
    "                p_tag = span_tag.find_next(\"p\")  # Find the next p tag after the span\n",
    "                if p_tag:\n",
    "                    #apm_participation_comment = p_tag.get_text(strip=True) if p_tag.text else \"<p> tag is empty\"\n",
    "                    apm_participation_comment = p_tag.get_text(strip=True) if p_tag.text else \"\" #return an empty string so that it looks better in the csv file                  \n",
    "                else:\n",
    "                    apm_participation_comment = \"No <p> tag found after the span\"\n",
    "            else:\n",
    "                apm_participation_comment = \"No <span> tag with class 'reporting-requirements no-label' found\"\n",
    "        else:\n",
    "            apm_participation_comment = \"No <h6> tag with 'APM Participation' found\"\n",
    "        #print(\"apm_participation_comment: \", apm_participation_comment)\n",
    "\n",
    "        ## Extract the APN Name (if it exists)\n",
    "        # Find the <h6> with 'APM Participation' and ensure it's the correct section\n",
    "        h6_tag = soup.find(\"h6\", text=lambda text: \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            section_tag = h6_tag.find_parent('section')  # Get the parent <section> of the <h6>\n",
    "            h7_tag = section_tag.find(\"div\", class_=\"h7\")  # Find the <div class=\"h7\"> within the section\n",
    "            if h7_tag:\n",
    "                apm_name = h7_tag.get_text(strip=True)  # Assign the text from <div class=\"h7\">\n",
    "            else:\n",
    "                #apm_name = \"No <div class='h7'> found within the section\"\n",
    "                apm_name = \"\" #return an empty string so it looks better in the csv file\n",
    "                \n",
    "        else:\n",
    "            apm_name = \"No <h6> with 'APM Participation' found\"\n",
    "        #print(\"apm_name: \", apm_name)\n",
    "\n",
    "\n",
    "        ## Extract (APM Details) Classification\n",
    "        # Find the <h6> with 'APM Participation' and ensure it's the correct section\n",
    "        h6_tag = soup.find(\"h6\", text=lambda text: \"APM Participation\" in text)\n",
    "        if h6_tag:\n",
    "            section_tag = h6_tag.find_parent('section')  # Get the parent <section> of the <h6>\n",
    "            details = section_tag.find_all(\"div\", class_=\"detail\")\n",
    "            for detail in details:\n",
    "                label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                if label and label.get_text(strip=True) == \"Classification\":\n",
    "                    detail_field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        apm_details_classification = detail_field.get_text(strip=True)  # Assign the text from <div class=\"detail-field\">\n",
    "                    break  # Stop after finding the first matching detail field\n",
    "        else:\n",
    "            apm_details_classification = \"No <h6> with 'APM Participation' found\"\n",
    "        #print(\"apm_details_classification: \", apm_details_classification)\n",
    "\n",
    "        ## Extract (APM Details) Model\n",
    "        # don't extract the h6_tag again. Let's see if we can use the same one as the precding block\n",
    "        if h6_tag:\n",
    "            section_tag = h6_tag.find_parent('section')  # Get the parent <section> of the <h6>\n",
    "            details = section_tag.find_all(\"div\", class_=\"detail\")\n",
    "            for detail in details:\n",
    "                label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                if label and label.get_text(strip=True) == \"Model\":\n",
    "                    detail_field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        apm_details_model = detail_field.get_text(strip=True)  # Assign the text from <div class=\"detail-field\">\n",
    "                    break  # Stop after finding the first matching detail field\n",
    "        else:\n",
    "            apm_details_model = \"No <h6> with 'APM Participation' found\"\n",
    "        #print(\"apm_details_model: \", apm_details_model)\n",
    "\n",
    "        ## Extract (APM Details) Participation Details\n",
    "        # don't extract the h6_tag again. Let's see if we can use the same one as the precding block\n",
    "        if h6_tag:\n",
    "            section_tag = h6_tag.find_parent('section')  # Get the parent <section> of the <h6>\n",
    "            details = section_tag.find_all(\"div\", class_=\"detail\")\n",
    "            for detail in details:\n",
    "                label = detail.find(\"div\", class_=\"detail-label\")\n",
    "                if label and label.get_text(strip=True) == \"Participation Details\":\n",
    "                    detail_field = detail.find(\"div\", class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        apm_details_participation_details = detail_field.get_text(strip=True)  # Assign the text from <div class=\"detail-field\">\n",
    "                    break  # Stop after finding the first matching detail field\n",
    "        else:\n",
    "            apm_details_participation_details = \"No <h6> with 'APM Participation' found\"\n",
    "        #print(\"apm_details_participation_details: \", apm_details_participation_details)\n",
    "\n",
    " \n",
    "        #### Extract (Clinician Level) SPECIAL STATUS Hospital-based\n",
    "         # Find the section 'Other Reporting Factors'\n",
    "        section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "\n",
    "        if section_header:\n",
    "            # Find the div that contains the 'Clinician Level' text, safely check for the div existence\n",
    "            scenario_div = section_header.find_next('div', class_=\"scenario\")\n",
    "            if scenario_div:\n",
    "                clinician_div = scenario_div.find('div', string=\"Clinician Level\")\n",
    "                if clinician_div and clinician_div.parent:\n",
    "                    # Find the span containing 'Hospital-based' and its corresponding 'div class=\"detail-field\"'\n",
    "                    hospital_based_span = clinician_div.parent.find('span', string=\"Hospital-based\")\n",
    "                    if hospital_based_span:\n",
    "                        detail_field = hospital_based_span.find_next('div', class_=\"detail-field\")\n",
    "                        if detail_field:\n",
    "                            cl_hospital_based_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        cl_hospital_based_status = \"No\"\n",
    "                else:\n",
    "                    cl_hospital_based_status = \"No\"\n",
    "            else:\n",
    "                cl_hospital_based_status = \"No\"\n",
    "        else:\n",
    "            cl_hospital_based_status = \"No\"\n",
    "\n",
    "        #print(\"cl_hospital_based_status:\", cl_hospital_based_status)\n",
    "                \n",
    "        #### Extract (Clinician Level) SPECIAL STATUS Non-patient facing\n",
    "        # Find the section 'Other Reporting Factors'\n",
    "        section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "\n",
    "        if section_header:\n",
    "            # Safely find the div that contains the 'Clinician Level' text\n",
    "            scenario_div = section_header.find_next('div', class_=\"scenario\")\n",
    "            clinician_div = scenario_div.find('div', string=\"Clinician Level\") if scenario_div else None\n",
    "    \n",
    "            if clinician_div and clinician_div.parent:\n",
    "                # Find the span containing 'Non-patient facing' and its corresponding 'div class=\"detail-field\"'\n",
    "                npfacing_span = clinician_div.parent.find('span', string=\"Non-patient facing\")\n",
    "                if npfacing_span:\n",
    "                    detail_field = npfacing_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        cl_npfacing_status = detail_field.text.strip()\n",
    "                else:\n",
    "                    cl_npfacing_status = \"No\"\n",
    "            else:\n",
    "                cl_npfacing_status = \"No\"\n",
    "        else:\n",
    "            cl_npfacing_status = \"No\"\n",
    "        #print(\"cl_npfacing_status:\", cl_npfacing_status)\n",
    "        \n",
    "        \n",
    "        ### Extract (Clinician Level) SPECIAL STATUS Rural\n",
    "        # Find the section 'Other Reporting Factors'\n",
    "        section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "        if section_header:\n",
    "            # Safely find the div that contains the 'Clinician Level' text\n",
    "            clinician_level_div = section_header.find_next('div', class_=\"scenario\")\n",
    "            clinician_div = clinician_level_div.find('div', string=\"Clinician Level\") if clinician_level_div else None\n",
    "    \n",
    "            if clinician_div and clinician_div.parent:\n",
    "                # Find the span containing 'Rural' and its corresponding 'div class=\"detail-field\"'\n",
    "                rural_span = clinician_div.parent.find('span', string=\"Rural\")\n",
    "                if rural_span:\n",
    "                    detail_field = rural_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        cl_rural_status = detail_field.text.strip()\n",
    "                else:\n",
    "                    cl_rural_status = \"No\"\n",
    "            else:\n",
    "                cl_rural_status = \"No\"\n",
    "        else:\n",
    "            cl_rural_status = \"Section 'Other Reporting Factors' not found.\"\n",
    "        #print(\"cl_rural_status:\", cl_rural_status)\n",
    "             \n",
    "        \n",
    "        ### Extract (Clinician Level) SPECIAL STATUS Small practice\n",
    "        # Find the section 'Other Reporting Factors'\n",
    "        section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "        if section_header:\n",
    "            # Find the div that contains the 'Clinician Level' text\n",
    "            scenario_div = section_header.find_next('div', class_=\"scenario\")\n",
    "            if scenario_div:\n",
    "                clinician_div = scenario_div.find('div', string=\"Clinician Level\")\n",
    "                if clinician_div and clinician_div.parent:\n",
    "                    # Find the span containing 'Small practice'\n",
    "                    small_practice_span = clinician_div.parent.find('span', string=\"Small practice\")\n",
    "                    if small_practice_span:\n",
    "                        detail_field = small_practice_span.find_next('div', class_=\"detail-field\")\n",
    "                        if detail_field:\n",
    "                            cl_small_practice_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        cl_small_practice_status = \"No\"\n",
    "                else:\n",
    "                    cl_small_practice_status = \"No\"\n",
    "            else:\n",
    "                cl_small_practice_status = \"No\"\n",
    "        else:\n",
    "            cl_small_practice_status = \"Section 'Other Reporting Factors' not found.\"\n",
    "        #print(\"cl_small_practice_status:\", cl_small_practice_status)\n",
    "\n",
    "       ### Extract (Clinician Level) SPECIAL STATUS Health Professional Shortage Area (HPSA)\n",
    "       # Find the section 'Other Reporting Factors'\n",
    "        section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "        if section_header:\n",
    "            # Find the div that contains the 'Clinician Level' text\n",
    "            scenario_div = section_header.find_next('div', class_=\"scenario\")\n",
    "            if scenario_div:\n",
    "                clinician_div = scenario_div.find('div', string=\"Clinician Level\")\n",
    "                if clinician_div and clinician_div.parent:\n",
    "                    # Find the span containing 'Small practice'\n",
    "                    small_practice_span = clinician_div.parent.find('span', string=\"Health Professional Shortage Area (HPSA)\")\n",
    "                    if small_practice_span:\n",
    "                        detail_field = small_practice_span.find_next('div', class_=\"detail-field\")\n",
    "                        if detail_field:\n",
    "                            cl_hspa_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        cl_hspa_status = \"No\"\n",
    "                else:\n",
    "                    cl_hspa_status = \"No\"\n",
    "            else:\n",
    "                cl_hspa_status = \"No\"\n",
    "        else:\n",
    "            cl_hspa_status = \"Section 'Other Reporting Factors' not found.\"\n",
    "        #print(\"cl_hspa_status:\", cl_hspa_status)\n",
    "\n",
    "        \n",
    "       ### Extract (Clinician Level) HARDSHIP EXCEPTION Extreme and uncontrollable circumstances\n",
    "        try:\n",
    "            # Find the section header to ensure we are working within the correct part of the document\n",
    "            section_header = soup.find('h6', string=\"Other Reporting Factors\")\n",
    "            if section_header:\n",
    "                # Navigate to the 'scenario' div, checking existence at each step\n",
    "                scenario_div = section_header.find_next_sibling('div', class_=\"scenario\")\n",
    "                if scenario_div:\n",
    "                    # Find the 'detail-boxed' div within 'scenario'\n",
    "                    detail_boxed = scenario_div.find('div', class_=\"detail-boxed\")\n",
    "                    if detail_boxed:\n",
    "                        # Locate the specific 'detail' div that contains the 'Hardship Exception' information\n",
    "                        detail_div = detail_boxed.find('div', class_=\"detail\")\n",
    "                        if detail_div:\n",
    "                            # Finally, find the 'detail-label' div and extract the text\n",
    "                            detail_label = detail_div.find('div', class_=\"detail-label\")\n",
    "                            if detail_label:\n",
    "                                cl_hardship = detail_label.get_text(strip=True)\n",
    "                                # Check if the extracted text matches the expected value\n",
    "                                if cl_hardship == \"Hardship ExceptionExtreme and uncontrollable circumstances\":\n",
    "                                    cl_hardship = \"Extreme and uncontrollable circumstances\"\n",
    "                                elif cl_hardship != \"Hardship ExceptionExtreme and uncontrollable circumstances\":\n",
    "                                    cl_hardship = \"No\"  # Reset to empty string if not matching\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            # cl_hardship remains an empty string if any exceptions are caught\n",
    "        #print(\"cl_hardship:\", cl_hardship)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Extract (Practice Level) SPECIAL STATUS Hospital-based       \n",
    "        # Attempt to find the section with 'Other Reporting Factors'\n",
    "        section = soup.find('h6', string=\"Other Reporting Factors\").parent if soup.find('h6', string=\"Other Reporting Factors\") else None       \n",
    "        if section:\n",
    "            # Attempt to find the 'Practice Level' div\n",
    "            practice_level_div = section.find('div', class_=\"h7\", string=\"Practice Level\")\n",
    "            if practice_level_div:\n",
    "                # Attempt to find the 'Hospital-based' span\n",
    "                hospital_based_span = practice_level_div.find_next('span', string=\"Hospital-based\")\n",
    "                if hospital_based_span:\n",
    "                    # Attempt to find the corresponding 'div class=\"detail-field\"'\n",
    "                    detail_field = hospital_based_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        pl_hospital_based_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        pass\n",
    "                        #pl_hospital_based_status = \"Detail field not found for 'Hospital-based'.\"\n",
    "                else:\n",
    "                    pass\n",
    "                    #pl_hospital_based_status = \"'Hospital-based' span not found.\"\n",
    "            else:\n",
    "                pass\n",
    "                #pl_hospital_based_status = \"'Practice Level' div not found.\"\n",
    "        else:\n",
    "            pl_hospital_based_status = \"'Other Reporting Factors' section not found.\"\n",
    "        #print(\"pl_hospital_based_status:\", pl_hospital_based_status)\n",
    "\n",
    "        \n",
    "        ### Extract (Practice Level) SPECIAL STATUS Rural       \n",
    "        # Attempt to find the section with 'Other Reporting Factors'\n",
    "        section = soup.find('h6', string=\"Other Reporting Factors\").parent if soup.find('h6', string=\"Other Reporting Factors\") else None       \n",
    "        if section:\n",
    "            # Attempt to find the 'Practice Level' div\n",
    "            practice_level_div = section.find('div', class_=\"h7\", string=\"Practice Level\")\n",
    "            if practice_level_div:\n",
    "                # Attempt to find the 'Hospital-based' span\n",
    "                hospital_based_span = practice_level_div.find_next('span', string=\"Rural\")\n",
    "                if hospital_based_span:\n",
    "                    # Attempt to find the corresponding 'div class=\"detail-field\"'\n",
    "                    detail_field = hospital_based_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        pl_rural_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        pass\n",
    "                        #pl_hospital_based_status = \"Detail field not found for 'Hospital-based'.\"\n",
    "                else:\n",
    "                    pass\n",
    "                    #pl_hospital_based_status = \"'Hospital-based' span not found.\"\n",
    "            else:\n",
    "                pass\n",
    "                #pl_hospital_based_status = \"'Practice Level' div not found.\"\n",
    "        else:\n",
    "            pl_rural_status = \"'Other Reporting Factors' section not found.\"\n",
    "        #print(\"pl_rural_status:\", pl_rural_status)\n",
    "        \n",
    "        ### Extract (Practice Level) SPECIAL STATUS Small practice       \n",
    "        # Attempt to find the section with 'Other Reporting Factors'\n",
    "        section = soup.find('h6', string=\"Other Reporting Factors\").parent if soup.find('h6', string=\"Other Reporting Factors\") else None       \n",
    "        if section:\n",
    "            # Attempt to find the 'Practice Level' div\n",
    "            practice_level_div = section.find('div', class_=\"h7\", string=\"Practice Level\")\n",
    "            if practice_level_div:\n",
    "                # Attempt to find the 'Hospital-based' span\n",
    "                hospital_based_span = practice_level_div.find_next('span', string=\"Small practice\")\n",
    "                if hospital_based_span:\n",
    "                    # Attempt to find the corresponding 'div class=\"detail-field\"'\n",
    "                    detail_field = hospital_based_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        pl_small_practice_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        pass\n",
    "                        #pl_hospital_based_status = \"Detail field not found for 'Hospital-based'.\"\n",
    "                else:\n",
    "                    pass\n",
    "                    #pl_hospital_based_status = \"'Hospital-based' span not found.\"\n",
    "            else:\n",
    "                pass\n",
    "                #pl_hospital_based_status = \"'Practice Level' div not found.\"\n",
    "        else:\n",
    "            pl_small_practice_status = \"'Other Reporting Factors' section not found.\"\n",
    "        #print(\"pl_small_practice_status:\", pl_small_practice_status)\n",
    "\n",
    "\n",
    "        ### Extract (Practice Level) SPECIAL STATUS Health Professional Shortage Area (HPSA)       \n",
    "        # Attempt to find the section with 'Other Reporting Factors'\n",
    "        section = soup.find('h6', string=\"Other Reporting Factors\").parent if soup.find('h6', string=\"Other Reporting Factors\") else None       \n",
    "        if section:\n",
    "            # Attempt to find the 'Practice Level' div\n",
    "            practice_level_div = section.find('div', class_=\"h7\", string=\"Practice Level\")\n",
    "            if practice_level_div:\n",
    "                # Attempt to find the 'Hospital-based' span\n",
    "                hospital_based_span = practice_level_div.find_next('span', string=\"Health Professional Shortage Area (HPSA)\")\n",
    "                if hospital_based_span:\n",
    "                    # Attempt to find the corresponding 'div class=\"detail-field\"'\n",
    "                    detail_field = hospital_based_span.find_next('div', class_=\"detail-field\")\n",
    "                    if detail_field:\n",
    "                        pl_hpsa_status = detail_field.text.strip()\n",
    "                    else:\n",
    "                        pass\n",
    "                        #pl_hospital_based_status = \"Detail field not found for 'Hospital-based'.\"\n",
    "                else:\n",
    "                    pass\n",
    "                    #pl_hospital_based_status = \"'Hospital-based' span not found.\"\n",
    "            else:\n",
    "                pass\n",
    "                #pl_hospital_based_status = \"'Practice Level' div not found.\"\n",
    "        else:\n",
    "            pl_hpsa_status = \"'Other Reporting Factors' section not found.\"\n",
    "        #print(\"pl_hpsa_status:\", pl_hpsa_status)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return [npi, data_fetch_status, year, associated_practices, physician_name, practice_name, practice_address, individual_status, group_status, optin_text, mips_reporting_requirements, mips_reporting_options, payment_information, cl_exceeds_low_volume, cl_med_patients, cl_allowed_charges, cl_covered_services, cl_mips_ec_type, cl_enroll_b4_2024, pl_exceed_lv, pl_medicare_pts, pl_allowed_charges, pl_covered_services, apm_participation_num, apm_participation_text, apm_participation_comment, apm_name, apm_details_classification, apm_details_model, apm_details_participation_details, cl_hospital_based_status, cl_npfacing_status, cl_rural_status, cl_small_practice_status, cl_hspa_status, cl_hardship, pl_hospital_based_status, pl_rural_status, pl_small_practice_status, pl_hpsa_status]\n",
    "        #return [npi, associated_practices, physician_name, practice_name, practice_address, mips_info]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for NPI {npi}: {str(e)}\")\n",
    "        #return [npi, 'Error', 'Error', 'Error', 'Error']\n",
    "        return [npi, str(e)]\n",
    "\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "## These are the variables to tune the way the program runs\n",
    "##\n",
    "##\n",
    "max_fetch_attempts = 5  # Sometimes the qpp page returns that 2024 is not a supported year. \n",
    "                        # This appears to be a glitch in their system because when you check\n",
    "                        # it by plugging the NPI in by hand, you get good data. \n",
    "                        # This variable sets how many times you want to retry an HTML pull for that NPI\n",
    "                        # before giving up. If it fails to read NPI data after max tries times, you'll see a\n",
    "                        # \"Failed: Unsupported performance year after manimum retries\" in the output file\n",
    "                        # for that NPI\n",
    "\n",
    "# Variable to specify the input filename for the list of the NPI numbers to read. \n",
    "#input_filename = 'Some Other Filename That You Have.txt'  # You can set this variable to any filename as required\n",
    "input_filename = 'NPI-4.txt'  # You can set this variable to any filename as required\n",
    "\n",
    "## The QPP Site has data for years 2022, 2023, and 2024. For each NPI in input_filename, the program will read \n",
    "## each of the years specified in years.\n",
    "years = [2022, 2023, 2024]  # List of years\n",
    "#years = [2024]  # Uncomment this one to extract data from 2024 only\n",
    "\n",
    "# Read NPI numbers from file\n",
    "#with open('NPI.txt', 'r') as file:\n",
    "with open(input_filename, 'r') as file:\n",
    "    print(\"Opening the NPI text file\")\n",
    "    npi_numbers = [line.strip() for line in file]\n",
    "    # Using list comprehension to filter out empty strings\n",
    "    npi_numbers = [npi for npi in npi_numbers if npi]\n",
    "    #print(\"npi_numbers = \", npi_numbers)\n",
    "    #print()\n",
    "    cleaned_npi_numbers = []  # List to hold cleaned NPI numbers\n",
    "    for npi in npi_numbers:\n",
    "        if not npi.isascii():  # Check if the NPI contains non-ASCII characters\n",
    "            original_npi = npi  # Store the original NPI for reference\n",
    "            cleaned_npi = re.sub(r'[^\\x00-\\x7F]+', '', npi)  # Remove non-ASCII characters using regex\n",
    "            cleaned_npi_numbers.append(cleaned_npi)\n",
    "            print(f\"Non-ASCII characters were found in NPI {original_npi}. Cleaned NPI: {cleaned_npi}\")\n",
    "        else:\n",
    "            cleaned_npi_numbers.append(npi)  # If no non-ASCII characters, add the original NPI to the list\n",
    "\n",
    "    # Calculate the number of elements in the list\n",
    "    number_of_npi = len(cleaned_npi_numbers)\n",
    "    print(\"Number of NPI numbers:\", number_of_npi)\n",
    "    print()\n",
    "\n",
    "# Prepare CSV output\n",
    "    #print(\"Opening the output .csv file\")\n",
    "\n",
    "    # Get the current datetime formatted as YYYYMMDDhhmmss\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "# Dynamically generate the output filename based on the input filename\n",
    "output_filename = f'{current_time} {input_filename.replace(\".txt\", \"\")} REPORT.csv'  \n",
    "years = [2022, 2023, 2024]  # List of years\n",
    "#years = [2024]  # List of years\n",
    "number_of_years = len(years)\n",
    "number_of_npi_counting_years = number_of_npi*number_of_years\n",
    "remaining_npi = number_of_npi*number_of_years # seed remaining_npi\n",
    "processed_npi = 0 # seed the number of npi processed to 0\n",
    "#print(\"number_of_npi_counting_years:\", number_of_npi_counting_years)\n",
    "\n",
    "\n",
    "#with open('MIPS REPORTING PER NPI.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['NPI', 'Data Fetch Status', 'Year','Associated Practices', 'Provider Name', 'Practice Name', 'Practice Address', 'MIPS Eligibility: Individual', 'MIPS Eligibility: Group', 'Opt-in Option', 'MIPS Reporting Requirements', 'MIPS Reporting & Participating Options', 'Payment Information', '(Clinician Level) Exceeds low volume threshold', '(Clinician Level) Medicare patients for this clinician', '(Clinician Level) Allowed charges for this clinician', '(Clinician Level) Covered services for this clinician', '(Clinician Level) MIPS eligible clinician type', '(Clinician Level) Enrolled in Medicare before January 1, 2024', '(Practice Level) Exceeds low volume threshold', '(Practice Level) Medicare patients at this practice', '(Practice Level) Allowed charges at this practice', '(Practice Level) Covered services at this practice', 'APM Participation', 'APM Participation Requirements', 'APM Participation Requirements Comment', 'APM Name', '(APM Details) Classification', '(APM Details) Model', '(APM Details) Participation Details', '(Clinician Level) SPECIAL STATUS Hospital-based', '(Clinician Level) SPECIAL STATUS Non-patient facing', '(Clinician Level) SPECIAL STATUS Rural', '(Clinician Level) SPECIAL STATUS Small Practice', '(Clinician Level) Health Professional Shortage Area (HPSA)', '(Clinician Level) (HARDSHIP EXCEPTION)','(Practice Level) SPECIAL STATUS Hospital-based', '(Practice Level) SPECIAL STATUS Rural', '(Practice Level) SPECIAL STATUS Small Practice', '(Practice Level) Health Professional Shortage Area (HPSA)'])\n",
    " \n",
    "    total_time = 0\n",
    "    # Fetch and write data for each NPI\n",
    "    for i, npi in enumerate(cleaned_npi_numbers, 1):  # Using enumerate to track index starting from 1\n",
    " \n",
    "        for year in years:\n",
    "            #max_fetch_attempts = 3\n",
    "            fetch_attempt = 0\n",
    "            data = []\n",
    "                   \n",
    "            while fetch_attempt < max_fetch_attempts:\n",
    "                start_time = time.time()\n",
    "                print(f\"Beginning HTML scrape for NPI {npi} for the year {year}\")\n",
    "                data = fetch_info(npi, year) \n",
    "                #print(\"data: \", data)\n",
    "                print()\n",
    "                print()\n",
    "                \n",
    "                # Check if data contains the unsupported year message\n",
    "                error_message = \"is not a supported performance year\"\n",
    "                error_found = any(error_message in str(item) for item in data)  # Convert items to string to safely use 'in'\n",
    "                if error_found:\n",
    "                    print(\"Error message found!\")\n",
    "                    print(f\"Attempt {fetch_attempt + 1}: Unsupported performance year found. Retrying...\")\n",
    "                    fetch_attempt += 1\n",
    "                    if fetch_attempt == max_fetch_attempts:\n",
    "                        print(f\"Maximum attempts reached for NPI {npi} for the year {year}. Proceeding with the next.\")\n",
    "                        data = [npi, \"Failed: Unsupported performance year after maximum retries\", year]\n",
    "                        break\n",
    "                    continue  # Continue to retry\n",
    "                else:\n",
    "                    # If no error related to unsupported year, exit the loop\n",
    "                    print(\"Did not detect Not a Supported Performance Year\")\n",
    "                    print()\n",
    "                    break\n",
    "                                       \n",
    "                      \n",
    "            print(\"Data to write to csv =\", data)\n",
    "            writer.writerow(data)\n",
    "            #print(\"Scrape complete for NPI\", npi)\n",
    "            print(\"Scrape complete for NPI\", npi, \"for the year\", year)\n",
    "  \n",
    "            duration = time.time() - start_time\n",
    "            total_time += duration\n",
    "       \n",
    "            # Calculate average time per NPI and estimate remaining time\n",
    "            processed_npi = processed_npi + 1 # incrememnt the number of NPI processed\n",
    "            remaining_npi = number_of_npi_counting_years - processed_npi\n",
    "\n",
    "            print(\"processed_npi: \", processed_npi)\n",
    "            print(\"remaining_npi: \", remaining_npi)\n",
    " \n",
    "            #average_time = total_time / i\n",
    "            average_time = total_time / processed_npi\n",
    "\n",
    "            estimated_remaining_time = average_time * remaining_npi\n",
    "            # Format remaining time in minutes\n",
    "            estimated_remaining_time_minutes = estimated_remaining_time / 60\n",
    "\n",
    "            # Store calculations in variables before printing\n",
    "            #processed = i\n",
    "            #remaining = number_of_npi - i\n",
    "            average_time_formatted = f\"{average_time:.2f}\"\n",
    "            estimated_remaining_time_formatted = f\"{estimated_remaining_time_minutes:.2f}\"\n",
    "\n",
    "            # Print the number of NPI numbers processed and remaining\n",
    "            print(f\"Processed: {processed_npi}/{number_of_npi_counting_years}, Remaining: {remaining_npi}\")\n",
    "            print(f\"Average processing time per NPI: {average_time_formatted} seconds\")\n",
    "            print(f\"Estimated remaining time: {estimated_remaining_time_formatted} minutes\")\n",
    "            print()        \n",
    "\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "print(\"Program Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76187f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
